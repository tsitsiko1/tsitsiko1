# -*- coding: utf-8 -*-
"""Nesi

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QRJqMqZ1YJ-mOK05sSVU99ofuBbz10KM
"""

#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""<a href="https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%204%20-%20Lesson%202%20-%20Notebook.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>"""

import re, collections
import nltk
from itertools import product
from collections import OrderedDict
import pandas as pd
import numpy as np

import tensorflow as tf

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
import numpy as np

#!wget --no-check-certificate \
   # https://storage.googleapis.com/laurencemoroney-blog.appspot.com/irish-lyrics-eof.txt \
  #  -O /tmp/irish-lyrics-eof.txt

from google.colab import files
files.upload()

# Read, then decode for py2 compat.
text = open('nesi.txt').read()
# length of text is the number of characters in it
print(f'Length of text: {len(text)} characters')

# Take a look at the first 250 characters in text
print(text[:250])

chars = tf.strings.unicode_split(text, input_encoding='UTF-8')
chars

# The unique characters in the file
vocab = sorted(set(text))
print(f'{len(vocab)} unique characters')

ids_from_chars = tf.keras.layers.StringLookup(
    vocabulary=list(vocab), mask_token=None)

ids = ids_from_chars(chars)
ids

chars_from_ids = tf.keras.layers.StringLookup(
    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)

from nltk.corpus.reader.plaintext import PlaintextCorpusReader



chars = tf.strings.unicode_split(text, input_encoding='UTF-8')
chars

ids_from_chars = tf.keras.layers.StringLookup(
    vocabulary=list(vocab), mask_token=None)

ids = ids_from_chars(chars)
ids

chars_from_ids = tf.keras.layers.StringLookup(
    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)

chars = chars_from_ids(ids)
chars

tf.strings.reduce_join(chars, axis=-1).numpy()

def text_from_ids(ids):
  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)



from mosestokenizer import *
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm 
import utils

tokenizer = MosesTokenizer()
N = 5
from google.colab import files
files.upload()

data_df = pd.read_csv('nesi.csv', lineterminator='\n', nrows = utils.NROWS)
data_df.shape, data_df.head()

! pip install FastText

import fasttext.util
ft = fasttext.load_model('wiki.ka.bin')
ft.get_dimension()

fasttext.util.reduce_model(ft, 100)
ft.get_dimension()

import io

def load_vectors(fname):
    fin = io.open(fname, 'r', encoding='utf-8', newline='\n', errors='ignore')
    n, d = map(int, fin.readline().split())
    data = {}
    for line in fin:
        tokens = line.rstrip().split(' ')
        data[tokens[0]] = map(float, tokens[1:])
    return data

my_string = "წლების უნახავი ნესი იმ დილით სამჯერ ვნახე."
PATTERN = r"\w+"
tokenizer = re.findall(PATTERN, my_string)

corpus = open('nesi.txt').read()
PATTERN = r"\w+"
#re.findall(PATTERN, text)

tokenizer = re.findall(PATTERN, my_string)





input_sequences = []
for line in corpus:
	token_list = tokenizer.text_to_sequences([line])[0]
	for i in range(1, len(token_list)):
		n_gram_sequence = token_list[:i+1]
		input_sequences.append(n_gram_sequence)

# pad sequences 
max_sequence_len = max([len(x) for x in input_sequences])
input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))

# create predictors and label
xs, labels = input_sequences[:,:-1],input_sequences[:,-1]

ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)

print(tokenizer.word_index['ის'])
#print(tokenizer.word_index['the'])
#print(tokenizer.word_index['town'])
#print(tokenizer.word_index['of'])
#print(tokenizer.word_index['athy'])
#print(tokenizer.word_index['one'])
#print(tokenizer.word_index['jeremy'])
#print(tokenizer.word_index['lanigan'])

print(xs[6])

print(ys[6])

print(xs[5])
print(ys[5])

print(tokenizer.word_index)

model = Sequential()
model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))
model.add(Bidirectional(LSTM(150)))
model.add(Dense(total_words, activation='softmax'))
adam = Adam(lr=0.01)
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')
history = model.fit(xs, ys, epochs=50, verbose=1)
#print model.summary()
print(model)

import matplotlib.pyplot as plt


def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.show()

plot_graphs(history, 'accuracy')

predict_x=model.predict(X_test) 
classes_x=np.argmax(predict_x,axis=1)



raw = open("nesi.txt").read()


tokenizer = re.findall(PATTERN, my_string)
txt = pd.Series(raw)

def tkn(txt):
    return re.findall('[a-z]+', txt.lower())

seed_text = "come"
next_words = 100
  
for _ in range(next_words):
	token_list = tokenizer.texts_to_sequences([seed_text])[0]
	token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
	predicted = model.predict_classes(token_list, verbose=0)
	output_word = ""
	for word, index in tokenizer.word_index.items():
		if index == predicted:
			output_word = word
			break
	seed_text += " " + output_word
print(seed_text)
